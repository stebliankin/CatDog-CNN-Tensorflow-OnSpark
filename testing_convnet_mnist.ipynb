{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stebliankin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import path to working directory\n",
    "workdir=\"/Users/stebliankin/Desktop/Data Science-CAP5768/project/CatDog-CNN-Tensorflow-OnSpark\"\n",
    "sys.path.append(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import local functions:\n",
    "import utils_MNIST\n",
    "import image_op\n",
    "import conv_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded /Users/stebliankin/Desktop/Data Science-CAP5768/project/CatDog-CNN-Tensorflow-OnSpark/data/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded /Users/stebliankin/Desktop/Data Science-CAP5768/project/CatDog-CNN-Tensorflow-OnSpark/data/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded /Users/stebliankin/Desktop/Data Science-CAP5768/project/CatDog-CNN-Tensorflow-OnSpark/data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Successfully downloaded /Users/stebliankin/Desktop/Data Science-CAP5768/project/CatDog-CNN-Tensorflow-OnSpark/data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Download MNIST data using utils_MNIST\n",
    "# MNIST dataset: http://yann.lecun.com/exdb/mnist/\n",
    "# Contains 60,000 training examples of hand written digits\n",
    "\n",
    "# Downloading MNIST data using API from stanford-tensorflow-tutorial:\n",
    "# https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/examples/utils.py\n",
    "os.mkdir(workdir+\"/data\")\n",
    "utils_MNIST.download_mnist(workdir+\"/data\"+\"/mnist\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make subclass of Mnist Convolutional Neural Network\n",
    "# Architecture of the network is specified in conv_net.py\n",
    "\n",
    "class MnistConvNet(conv_net.ConvNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.n_classes = 10\n",
    "        self.skip_step = 1  # printing rate\n",
    "\n",
    "    def get_data(self):\n",
    "        with tf.name_scope(\"mnist_data\"):\n",
    "            train_data, test_data = utils_MNIST.get_mnist_dataset(self.batch_size)\n",
    "            iterator = tf.data.Iterator.from_structure(train_data.output_types, train_data.output_shapes)\n",
    "            img, self.label = iterator.get_next()\n",
    "            self.img = tf.reshape(img, shape=[-1, 28, 28, 1])\n",
    "            self.train_init = iterator.make_initializer(train_data)\n",
    "            self.test_init = iterator.make_initializer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start program\n",
      "building a model\n",
      "data/mnist/train-images-idx3-ubyte.gz already exists\n",
      "data/mnist/train-labels-idx1-ubyte.gz already exists\n",
      "data/mnist/t10k-images-idx3-ubyte.gz already exists\n",
      "data/mnist/t10k-labels-idx1-ubyte.gz already exists\n",
      "WARNING:tensorflow:From /Users/stebliankin/Desktop/Data Science-CAP5768/project/CatDog-CNN-Tensorflow-OnSpark/conv_net.py:88: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "INFO:tensorflow:Summary name histogram loss is illegal; using histogram_loss instead.\n",
      "training\n",
      "Running session\n",
      "Variables initialized\n",
      "start training\n",
      "Loss at step 0: 2.3461496829986572\n",
      "Loss at step 1: 2.2322397232055664\n",
      "Loss at step 2: 2.1596157550811768\n",
      "Loss at step 3: 2.107093572616577\n",
      "Loss at step 4: 2.0383567810058594\n",
      "Loss at step 5: 1.9527785778045654\n",
      "Loss at step 6: 1.8467276096343994\n",
      "Loss at step 7: 1.7157974243164062\n",
      "Loss at step 8: 1.727343201637268\n",
      "Loss at step 9: 1.5459778308868408\n",
      "Loss at step 10: 1.5593876838684082\n",
      "Loss at step 11: 1.2825446128845215\n",
      "Loss at step 12: 1.2834157943725586\n",
      "Loss at step 13: 1.1970415115356445\n",
      "Loss at step 14: 0.9614909291267395\n",
      "Loss at step 15: 0.997925877571106\n",
      "Loss at step 16: 1.0093612670898438\n",
      "Loss at step 17: 0.9367520213127136\n",
      "Loss at step 18: 0.5812703967094421\n",
      "Loss at step 19: 0.7426226139068604\n",
      "Loss at step 20: 0.6918998956680298\n",
      "Loss at step 21: 0.6028244495391846\n",
      "Loss at step 22: 0.6120073795318604\n",
      "Loss at step 23: 0.6095749735832214\n",
      "Loss at step 24: 0.3814288377761841\n",
      "Loss at step 25: 0.668160617351532\n",
      "Loss at step 26: 0.4673369526863098\n",
      "Loss at step 27: 0.6185427308082581\n",
      "Loss at step 28: 0.5019676089286804\n",
      "Loss at step 29: 0.6255403757095337\n",
      "Loss at step 30: 0.523110568523407\n",
      "Loss at step 31: 0.5697700381278992\n",
      "Loss at step 32: 0.5576764345169067\n",
      "Loss at step 33: 0.33004236221313477\n",
      "Loss at step 34: 0.5555842518806458\n",
      "Loss at step 35: 0.2930397689342499\n",
      "Loss at step 36: 0.5296470522880554\n",
      "Loss at step 37: 0.3833175301551819\n",
      "Loss at step 38: 0.43533197045326233\n",
      "Loss at step 39: 0.33205121755599976\n",
      "Loss at step 40: 0.3384178578853607\n",
      "Loss at step 41: 0.3555019497871399\n",
      "Loss at step 42: 0.3748263716697693\n",
      "Loss at step 43: 0.31798309087753296\n",
      "Loss at step 44: 0.30159202218055725\n",
      "Loss at step 45: 0.3797180652618408\n",
      "Loss at step 46: 0.4070233106613159\n",
      "Loss at step 47: 0.2979885935783386\n",
      "Loss at step 48: 0.49970543384552\n",
      "Loss at step 49: 0.47325706481933594\n",
      "Loss at step 50: 0.44076067209243774\n",
      "Loss at step 51: 0.41933679580688477\n",
      "Loss at step 52: 0.5316925048828125\n",
      "Loss at step 53: 0.24400627613067627\n",
      "Loss at step 54: 0.27999618649482727\n",
      "Loss at step 55: 0.20667538046836853\n",
      "Loss at step 56: 0.2838696539402008\n",
      "Loss at step 57: 0.3216894567012787\n",
      "Loss at step 58: 0.3259463310241699\n",
      "Loss at step 59: 0.44341880083084106\n",
      "Loss at step 60: 0.275919109582901\n",
      "Loss at step 61: 0.3056366443634033\n",
      "Loss at step 62: 0.3336890935897827\n",
      "Loss at step 63: 0.245383620262146\n",
      "Loss at step 64: 0.24710285663604736\n",
      "Loss at step 65: 0.34974709153175354\n",
      "Loss at step 66: 0.14171859622001648\n",
      "Loss at step 67: 0.26043859124183655\n",
      "Loss at step 68: 0.28023210167884827\n",
      "Loss at step 69: 0.27935755252838135\n",
      "Loss at step 70: 0.34053555130958557\n",
      "Loss at step 71: 0.3523622155189514\n",
      "Loss at step 72: 0.25156155228614807\n",
      "Loss at step 73: 0.20871266722679138\n",
      "Loss at step 74: 0.23272183537483215\n",
      "Loss at step 75: 0.20350350439548492\n",
      "Loss at step 76: 0.2472863346338272\n",
      "Loss at step 77: 0.17969664931297302\n",
      "Loss at step 78: 0.381482869386673\n",
      "Loss at step 79: 0.2137647122144699\n",
      "Loss at step 80: 0.27758121490478516\n",
      "Loss at step 81: 0.24483518302440643\n",
      "Loss at step 82: 0.2415703684091568\n",
      "Loss at step 83: 0.2893698215484619\n",
      "Loss at step 84: 0.1867387890815735\n",
      "Loss at step 85: 0.20451714098453522\n",
      "Loss at step 86: 0.22703683376312256\n",
      "Loss at step 87: 0.15571139752864838\n",
      "Loss at step 88: 0.22003322839736938\n",
      "Loss at step 89: 0.2393411099910736\n",
      "Loss at step 90: 0.17187891900539398\n",
      "Loss at step 91: 0.31496867537498474\n",
      "Loss at step 92: 0.34784823656082153\n",
      "Loss at step 93: 0.3273863196372986\n",
      "Loss at step 94: 0.3170710802078247\n",
      "Loss at step 95: 0.3660939931869507\n",
      "Loss at step 96: 0.255405068397522\n",
      "Loss at step 97: 0.1716056913137436\n",
      "Loss at step 98: 0.17005065083503723\n",
      "Loss at step 99: 0.32986027002334595\n",
      "Loss at step 100: 0.33934393525123596\n",
      "Loss at step 101: 0.21032550930976868\n",
      "Loss at step 102: 0.20290082693099976\n",
      "Loss at step 103: 0.18841812014579773\n",
      "Loss at step 104: 0.19563817977905273\n",
      "Loss at step 105: 0.18167859315872192\n",
      "Loss at step 106: 0.2411794662475586\n",
      "Loss at step 107: 0.22278884053230286\n",
      "Loss at step 108: 0.21017000079154968\n",
      "Loss at step 109: 0.1777539998292923\n",
      "Loss at step 110: 0.24690616130828857\n",
      "Loss at step 111: 0.23147206008434296\n",
      "Loss at step 112: 0.15490195155143738\n",
      "Loss at step 113: 0.20308354496955872\n",
      "Loss at step 114: 0.17755815386772156\n",
      "Loss at step 115: 0.17301997542381287\n",
      "Loss at step 116: 0.29021668434143066\n",
      "Loss at step 117: 0.14317084848880768\n",
      "Loss at step 118: 0.13076913356781006\n",
      "Loss at step 119: 0.2256341576576233\n",
      "Loss at step 120: 0.21616283059120178\n",
      "Loss at step 121: 0.11426007002592087\n",
      "Loss at step 122: 0.2589353322982788\n",
      "Loss at step 123: 0.1771678626537323\n",
      "Loss at step 124: 0.38215598464012146\n",
      "Loss at step 125: 0.22332532703876495\n",
      "Loss at step 126: 0.19549034535884857\n",
      "Loss at step 127: 0.3064534664154053\n",
      "Loss at step 128: 0.18875354528427124\n",
      "Loss at step 129: 0.15842756628990173\n",
      "Loss at step 130: 0.25331565737724304\n",
      "Loss at step 131: 0.16030076146125793\n",
      "Loss at step 132: 0.09868443012237549\n",
      "Loss at step 133: 0.25720322132110596\n",
      "Loss at step 134: 0.2638910710811615\n",
      "Loss at step 135: 0.17580869793891907\n",
      "Loss at step 136: 0.19206669926643372\n",
      "Loss at step 137: 0.17308345437049866\n",
      "Loss at step 138: 0.0874248743057251\n",
      "Loss at step 139: 0.1586870402097702\n",
      "Loss at step 140: 0.27759498357772827\n",
      "Loss at step 141: 0.14189870655536652\n",
      "Loss at step 142: 0.22400137782096863\n",
      "Loss at step 143: 0.10165059566497803\n",
      "Loss at step 144: 0.194830983877182\n",
      "Loss at step 145: 0.18357381224632263\n",
      "Loss at step 146: 0.1955416202545166\n",
      "Loss at step 147: 0.09667263925075531\n",
      "Loss at step 148: 0.2645469605922699\n",
      "Loss at step 149: 0.097528837621212\n",
      "Loss at step 150: 0.17500236630439758\n",
      "Loss at step 151: 0.2829743027687073\n",
      "Loss at step 152: 0.13159938156604767\n",
      "Loss at step 153: 0.16031017899513245\n",
      "Loss at step 154: 0.2466590702533722\n",
      "Loss at step 155: 0.17090865969657898\n",
      "Loss at step 156: 0.17200690507888794\n",
      "Loss at step 157: 0.13403764367103577\n",
      "Loss at step 158: 0.23221494257450104\n",
      "Loss at step 159: 0.1544533669948578\n",
      "Loss at step 160: 0.23631645739078522\n",
      "Loss at step 161: 0.21666815876960754\n",
      "Loss at step 162: 0.13315466046333313\n",
      "Loss at step 163: 0.12300073355436325\n",
      "Loss at step 164: 0.11176463216543198\n",
      "Loss at step 165: 0.12602774798870087\n",
      "Loss at step 166: 0.1860424131155014\n",
      "Loss at step 167: 0.1775083839893341\n",
      "Loss at step 168: 0.24742403626441956\n",
      "Loss at step 169: 0.14425666630268097\n",
      "Loss at step 170: 0.17677214741706848\n",
      "Loss at step 171: 0.06571205705404282\n",
      "Loss at step 172: 0.14250412583351135\n",
      "Loss at step 173: 0.0924563854932785\n",
      "Loss at step 174: 0.23366111516952515\n",
      "Loss at step 175: 0.07048827409744263\n",
      "Loss at step 176: 0.1691991537809372\n",
      "Loss at step 177: 0.1237412691116333\n",
      "Loss at step 178: 0.15378764271736145\n",
      "Loss at step 179: 0.14055119454860687\n",
      "Loss at step 180: 0.13968899846076965\n",
      "Loss at step 181: 0.1451033055782318\n",
      "Loss at step 182: 0.14971646666526794\n",
      "Loss at step 183: 0.1448991894721985\n",
      "Loss at step 184: 0.053578440099954605\n",
      "Loss at step 185: 0.1275547593832016\n",
      "Loss at step 186: 0.08740673214197159\n",
      "Loss at step 187: 0.25785592198371887\n",
      "Loss at step 188: 0.11629930138587952\n",
      "Loss at step 189: 0.10055011510848999\n",
      "Loss at step 190: 0.1573140025138855\n",
      "Loss at step 191: 0.12966413795948029\n",
      "Loss at step 192: 0.23030304908752441\n",
      "Loss at step 193: 0.1598118096590042\n",
      "Loss at step 194: 0.12632903456687927\n",
      "Loss at step 195: 0.1326172649860382\n",
      "Loss at step 196: 0.17728032171726227\n",
      "Loss at step 197: 0.13221226632595062\n",
      "Loss at step 198: 0.09122385084629059\n",
      "Loss at step 199: 0.1291341632604599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 200: 0.21663184463977814\n",
      "Loss at step 201: 0.17924150824546814\n",
      "Loss at step 202: 0.1040247231721878\n",
      "Loss at step 203: 0.1167449951171875\n",
      "Loss at step 204: 0.17923544347286224\n",
      "Loss at step 205: 0.15166720747947693\n",
      "Loss at step 206: 0.10298560559749603\n",
      "Loss at step 207: 0.18532690405845642\n",
      "Loss at step 208: 0.16458940505981445\n",
      "Loss at step 209: 0.19695180654525757\n",
      "Loss at step 210: 0.10009614378213882\n",
      "Loss at step 211: 0.14505624771118164\n",
      "Loss at step 212: 0.15224692225456238\n",
      "Loss at step 213: 0.11710817366838455\n",
      "Loss at step 214: 0.1473563313484192\n",
      "Loss at step 215: 0.12357605993747711\n",
      "Loss at step 216: 0.13983865082263947\n",
      "Loss at step 217: 0.14291483163833618\n",
      "Loss at step 218: 0.20268596708774567\n",
      "Loss at step 219: 0.10364866256713867\n",
      "Loss at step 220: 0.23038634657859802\n",
      "Loss at step 221: 0.0901050865650177\n",
      "Loss at step 222: 0.1331804245710373\n",
      "Loss at step 223: 0.18586251139640808\n",
      "Loss at step 224: 0.09338787198066711\n",
      "Loss at step 225: 0.2009235918521881\n",
      "Loss at step 226: 0.08958908915519714\n",
      "Loss at step 227: 0.1362287998199463\n",
      "Loss at step 228: 0.08949828892946243\n",
      "Loss at step 229: 0.10349410772323608\n",
      "Loss at step 230: 0.09771893918514252\n",
      "Loss at step 231: 0.13145816326141357\n",
      "Loss at step 232: 0.12094766646623611\n",
      "Loss at step 233: 0.10154692828655243\n",
      "Loss at step 234: 0.19335176050662994\n",
      "Loss at step 235: 0.24612104892730713\n",
      "Loss at step 236: 0.10189761221408844\n",
      "Loss at step 237: 0.12732067704200745\n",
      "Loss at step 238: 0.1667650043964386\n",
      "Loss at step 239: 0.11948426067829132\n",
      "Loss at step 240: 0.12867191433906555\n",
      "Loss at step 241: 0.09211575984954834\n",
      "Loss at step 242: 0.08663611114025116\n",
      "Loss at step 243: 0.15070278942584991\n",
      "Loss at step 244: 0.3201316297054291\n",
      "Loss at step 245: 0.0705467090010643\n",
      "Loss at step 246: 0.2568693459033966\n",
      "Loss at step 247: 0.16597110033035278\n",
      "Loss at step 248: 0.10615389794111252\n",
      "Loss at step 249: 0.176207035779953\n",
      "Loss at step 250: 0.09208976477384567\n",
      "Loss at step 251: 0.2157728672027588\n",
      "Loss at step 252: 0.1831071376800537\n",
      "Loss at step 253: 0.055336739867925644\n",
      "Loss at step 254: 0.13747623562812805\n",
      "Loss at step 255: 0.1390611231327057\n",
      "Loss at step 256: 0.10635517537593842\n",
      "Loss at step 257: 0.2631221413612366\n",
      "Loss at step 258: 0.148874431848526\n",
      "Loss at step 259: 0.11965128779411316\n",
      "Loss at step 260: 0.15315163135528564\n",
      "Loss at step 261: 0.13459213078022003\n",
      "Loss at step 262: 0.052240997552871704\n",
      "Loss at step 263: 0.09583118557929993\n",
      "Loss at step 264: 0.10340391099452972\n",
      "Loss at step 265: 0.1191624104976654\n",
      "Loss at step 266: 0.12146327644586563\n",
      "Loss at step 267: 0.04444083198904991\n",
      "Loss at step 268: 0.15727654099464417\n",
      "Loss at step 269: 0.12425055354833603\n",
      "Loss at step 270: 0.12328129261732101\n",
      "Loss at step 271: 0.1614704728126526\n",
      "Loss at step 272: 0.21410509943962097\n",
      "Loss at step 273: 0.07289278507232666\n",
      "Loss at step 274: 0.1275227665901184\n",
      "Loss at step 275: 0.13220719993114471\n",
      "Loss at step 276: 0.1121445894241333\n",
      "Loss at step 277: 0.13120165467262268\n",
      "Loss at step 278: 0.09079963713884354\n",
      "Loss at step 279: 0.0911283940076828\n",
      "Loss at step 280: 0.12249805778265\n",
      "Loss at step 281: 0.05377758666872978\n",
      "Loss at step 282: 0.07611750066280365\n",
      "Loss at step 283: 0.12943366169929504\n",
      "Loss at step 284: 0.16173699498176575\n",
      "Loss at step 285: 0.0969681516289711\n",
      "Loss at step 286: 0.13087768852710724\n",
      "Loss at step 287: 0.09208343923091888\n",
      "Loss at step 288: 0.07458905875682831\n",
      "Loss at step 289: 0.07685229182243347\n",
      "Loss at step 290: 0.1269814670085907\n",
      "Loss at step 291: 0.0760720744729042\n",
      "Loss at step 292: 0.11961142718791962\n",
      "Loss at step 293: 0.16622865200042725\n",
      "Loss at step 294: 0.08639884740114212\n",
      "Loss at step 295: 0.16571399569511414\n",
      "Loss at step 296: 0.09444241225719452\n",
      "Loss at step 297: 0.14195898175239563\n",
      "Loss at step 298: 0.19390249252319336\n",
      "Loss at step 299: 0.1885015368461609\n",
      "Loss at step 300: 0.12353632599115372\n",
      "Loss at step 301: 0.10867293179035187\n",
      "Loss at step 302: 0.23406828939914703\n",
      "Loss at step 303: 0.07457220554351807\n",
      "Loss at step 304: 0.12281599640846252\n",
      "Loss at step 305: 0.08499608188867569\n",
      "Loss at step 306: 0.12028855085372925\n",
      "Loss at step 307: 0.10417637228965759\n",
      "Loss at step 308: 0.10635460913181305\n",
      "Loss at step 309: 0.08556760847568512\n",
      "Loss at step 310: 0.02720356360077858\n",
      "Loss at step 311: 0.13739261031150818\n",
      "Loss at step 312: 0.07848259061574936\n",
      "Loss at step 313: 0.20720398426055908\n",
      "Loss at step 314: 0.07367739826440811\n",
      "Loss at step 315: 0.11968915164470673\n",
      "Loss at step 316: 0.10508044064044952\n",
      "Loss at step 317: 0.05351007729768753\n",
      "Loss at step 318: 0.08871983736753464\n",
      "Loss at step 319: 0.20663060247898102\n",
      "Loss at step 320: 0.15165266394615173\n",
      "Loss at step 321: 0.214869424700737\n",
      "Loss at step 322: 0.11668817698955536\n",
      "Loss at step 323: 0.06045520678162575\n",
      "Loss at step 324: 0.14741404354572296\n",
      "Loss at step 325: 0.07832784205675125\n",
      "Loss at step 326: 0.12056539952754974\n",
      "Loss at step 327: 0.20321276783943176\n",
      "Loss at step 328: 0.12768584489822388\n",
      "Loss at step 329: 0.12408454716205597\n",
      "Loss at step 330: 0.07856776565313339\n",
      "Loss at step 331: 0.06119358539581299\n",
      "Loss at step 332: 0.0810563713312149\n",
      "Loss at step 333: 0.09414651989936829\n",
      "Loss at step 334: 0.07553493976593018\n",
      "Loss at step 335: 0.07228747755289078\n",
      "Loss at step 336: 0.10517341643571854\n",
      "Loss at step 337: 0.1091507226228714\n",
      "Loss at step 338: 0.08217614889144897\n",
      "Loss at step 339: 0.07590696215629578\n",
      "Loss at step 340: 0.152106374502182\n",
      "Loss at step 341: 0.11713271588087082\n",
      "Loss at step 342: 0.08309244364500046\n",
      "Loss at step 343: 0.07684355974197388\n",
      "Loss at step 344: 0.09868752211332321\n",
      "Loss at step 345: 0.0701032280921936\n",
      "Loss at step 346: 0.12275389581918716\n",
      "Loss at step 347: 0.17015507817268372\n",
      "Loss at step 348: 0.08636683225631714\n",
      "Loss at step 349: 0.10710321366786957\n",
      "Loss at step 350: 0.1747075766324997\n",
      "Loss at step 351: 0.13592442870140076\n",
      "Loss at step 352: 0.025836437940597534\n",
      "Loss at step 353: 0.12293070554733276\n",
      "Loss at step 354: 0.10854682326316833\n",
      "Loss at step 355: 0.09137462079524994\n",
      "Loss at step 356: 0.10264816880226135\n",
      "Loss at step 357: 0.17986759543418884\n",
      "Loss at step 358: 0.10096114873886108\n",
      "Loss at step 359: 0.13222116231918335\n",
      "Loss at step 360: 0.10725956410169601\n",
      "Loss at step 361: 0.10768324881792068\n",
      "Loss at step 362: 0.1316351294517517\n",
      "Loss at step 363: 0.16625091433525085\n",
      "Loss at step 364: 0.14914482831954956\n",
      "Loss at step 365: 0.09932144731283188\n",
      "Loss at step 366: 0.08798639476299286\n",
      "Loss at step 367: 0.05854091793298721\n",
      "Loss at step 368: 0.17074207961559296\n",
      "Loss at step 369: 0.15838144719600677\n",
      "Loss at step 370: 0.10985805094242096\n",
      "Loss at step 371: 0.15269316732883453\n",
      "Loss at step 372: 0.07245641946792603\n",
      "Loss at step 373: 0.10386176407337189\n",
      "Loss at step 374: 0.14987139403820038\n",
      "Loss at step 375: 0.04711884260177612\n",
      "Loss at step 376: 0.08290494978427887\n",
      "Loss at step 377: 0.05741550773382187\n",
      "Loss at step 378: 0.05149943381547928\n",
      "Loss at step 379: 0.11083372682332993\n",
      "Loss at step 380: 0.1914646178483963\n",
      "Loss at step 381: 0.06956364214420319\n",
      "Loss at step 382: 0.1393679678440094\n",
      "Loss at step 383: 0.14932267367839813\n",
      "Loss at step 384: 0.06701363623142242\n",
      "Loss at step 385: 0.06727445870637894\n",
      "Loss at step 386: 0.15911045670509338\n",
      "Loss at step 387: 0.13495604693889618\n",
      "Loss at step 388: 0.08444244414567947\n",
      "Loss at step 389: 0.055124811828136444\n",
      "Loss at step 390: 0.11853411793708801\n",
      "Loss at step 391: 0.10299284756183624\n",
      "Loss at step 392: 0.10396091639995575\n",
      "Loss at step 393: 0.2599543631076813\n",
      "Loss at step 394: 0.0981813445687294\n",
      "Loss at step 395: 0.04482778161764145\n",
      "Loss at step 396: 0.09640433639287949\n",
      "Loss at step 397: 0.09524279087781906\n",
      "Loss at step 398: 0.16970443725585938\n",
      "Loss at step 399: 0.053576014935970306\n",
      "Loss at step 400: 0.09979043155908585\n",
      "Loss at step 401: 0.1129511371254921\n",
      "Loss at step 402: 0.05649340897798538\n",
      "Loss at step 403: 0.04468991607427597\n",
      "Loss at step 404: 0.1691831350326538\n",
      "Loss at step 405: 0.08733467757701874\n",
      "Loss at step 406: 0.05624808743596077\n",
      "Loss at step 407: 0.10051552951335907\n",
      "Loss at step 408: 0.28244948387145996\n",
      "Loss at step 409: 0.07310417294502258\n",
      "Loss at step 410: 0.03255802392959595\n",
      "Loss at step 411: 0.17725375294685364\n",
      "Loss at step 412: 0.32077667117118835\n",
      "Loss at step 413: 0.11748914420604706\n",
      "Loss at step 414: 0.16954942047595978\n",
      "Loss at step 415: 0.11230531334877014\n",
      "Loss at step 416: 0.22266963124275208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 417: 0.09720823168754578\n",
      "Loss at step 418: 0.09612728655338287\n",
      "Loss at step 419: 0.16111493110656738\n",
      "Loss at step 420: 0.13721629977226257\n",
      "Loss at step 421: 0.1441315859556198\n",
      "Loss at step 422: 0.12562912702560425\n",
      "Loss at step 423: 0.10143409669399261\n",
      "Loss at step 424: 0.06752309948205948\n",
      "Loss at step 425: 0.10629211366176605\n",
      "Loss at step 426: 0.09668680280447006\n",
      "Loss at step 427: 0.07562172412872314\n",
      "Loss at step 428: 0.07993283867835999\n",
      "Loss at step 429: 0.16552485525608063\n",
      "Average loss at epoch 0: 0.24219494393571864\n",
      "Took: 73.36825434764226 minutes\n",
      "Accuracy at epoch 0: 0.9761669303797469 \n",
      "Took: 5.612814903259277 seconds\n",
      "Done. Running time is 73.58290253480276 min.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start = time.time()\n",
    "print('start program')\n",
    "model = MnistConvNet()\n",
    "print('building a model')\n",
    "model.build()\n",
    "print('training')\n",
    "model.train(n_epochs=1)\n",
    "print(\"Done. Running time is {} min.\".format((time.time() - start)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
